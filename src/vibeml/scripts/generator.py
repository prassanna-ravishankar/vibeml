"""Script generation for training jobs."""

from typing import Dict, Any
from jinja2 import Template


class ScriptGenerator:
    """Generates optimized training scripts."""

    UNSLOTH_TEMPLATE = """#!/usr/bin/env python3
\"\"\"Training script generated by VibeML.\"\"\"

import torch
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments

# Configuration
MODEL_NAME = "{{ model }}"
DATASET_NAME = "{{ dataset }}"
MAX_SEQ_LENGTH = {{ max_seq_length }}
MAX_STEPS = {{ max_steps }}
LEARNING_RATE = {{ learning_rate }}
OUTPUT_DIR = "{{ output_dir }}"

def main():
    # Load model with 4-bit quantization
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_NAME,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,
        load_in_4bit=True,
    )

    # Prepare model for training
    model = FastLanguageModel.get_peft_model(
        model,
        r={{ lora_r }},
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_alpha=16,
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing=True,
    )

    # Load dataset
    dataset = load_dataset(DATASET_NAME, split="train")

    # Training arguments
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        max_steps=MAX_STEPS,
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        logging_steps=10,
        save_steps=100,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
    )

    # Initialize trainer
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        args=training_args,
        dataset_text_field="text",
        max_seq_length=MAX_SEQ_LENGTH,
    )

    # Train
    print("Starting training...")
    trainer.train()

    # Save model
    print(f"Saving model to {OUTPUT_DIR}")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print("Training complete!")

if __name__ == "__main__":
    main()
"""

    def generate_script(
        self,
        workflow: str,
        model: str,
        dataset: str,
        **kwargs: Any,
    ) -> str:
        """Generate training script.

        Args:
            workflow: Workflow type (unsloth, gpt-oss-lora, etc.)
            model: Model name
            dataset: Dataset name
            **kwargs: Additional parameters

        Returns:
            Generated Python script as string
        """
        if workflow == "unsloth":
            return self._generate_unsloth_script(model, dataset, **kwargs)

        # Default fallback
        return self._generate_unsloth_script(model, dataset, **kwargs)

    def _generate_unsloth_script(
        self,
        model: str,
        dataset: str,
        **kwargs: Any,
    ) -> str:
        """Generate Unsloth training script."""
        params = {
            "model": model,
            "dataset": dataset,
            "max_seq_length": kwargs.get("max_seq_length", 2048),
            "max_steps": kwargs.get("max_steps", 60),
            "learning_rate": kwargs.get("learning_rate", 2e-4),
            "lora_r": kwargs.get("lora_r", 16),
            "output_dir": kwargs.get("output_dir", "./outputs"),
        }

        template = Template(self.UNSLOTH_TEMPLATE)
        return template.render(**params)
